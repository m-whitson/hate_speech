{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech on Twitter: Scraping the Twitter API\n",
    "## Team 8: \n",
    " - Meera Whitson whitson.m@northeastern.edu\n",
    " - Anthony Bernardi bernardi.an@northeastern.edu\n",
    " \n",
    "### CONTENT WARNING: offensive language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "We are making calls to the Twitter API, which is run and maintained by Twitter. We have received developer licenses from Twitter, the keys for which are shown below. In our research into how to use the Twitter API, we found [this Python library called TwitterAPI](https://github.com/geduldig/TwitterAPI) (no space; \"Twitter API\" is the actual API, \"TwitterAPI\" is the name of the library), and we will be using it to make requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TwitterAPI\n",
      "  Downloading TwitterAPI-2.6.9.1.tar.gz (11 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from TwitterAPI) (2.24.0)\n",
      "Collecting requests_oauthlib\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->TwitterAPI) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->TwitterAPI) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->TwitterAPI) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->TwitterAPI) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: TwitterAPI\n",
      "  Building wheel for TwitterAPI (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for TwitterAPI: filename=TwitterAPI-2.6.9.1-py3-none-any.whl size=13040 sha256=5368d91d32804d98f8dc777ff336d91c774678170cccf0d8ceccf847182d428f\n",
      "  Stored in directory: /Users/meerawhitson/Library/Caches/pip/wheels/99/60/09/74d03ca2fdb4ef0891358249711f8939161265deba78a747cc\n",
      "Successfully built TwitterAPI\n",
      "Installing collected packages: oauthlib, requests-oauthlib, TwitterAPI\n",
      "Successfully installed TwitterAPI-2.6.9.1 oauthlib-3.1.0 requests-oauthlib-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "API keys and authorization objects. We're going to run it using the oAuth2, but we have the code for both if we want to replace it with oAuth1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my (Meera) API keys. I have a Twitter developer license which gives me access to the full archive (everything posted on Twitter)\n",
    "\n",
    "API_KEY = 'OKpfVrIxI2Eo266NTmr1wkHz6'\n",
    "API_KEY_SECRET = 'txzAyLMDCWEptv6OJyHei82m0s57O22nc6ZWtzwRhetiS7Wwsh'\n",
    "ACCESS_TOKEN = '1033180687250153472-8nyEZ3RIsVWzQ0IYZ17o4P1I9IFnzh'\n",
    "ACCESS_TOKEN_SECRET = '6QWRdu6y3H6POnRcPUFDPrJuEXt2csGX5mSk8ECKZuTd5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests_oauthlib.oauth1_auth.OAuth1 at 0x115eed430>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## oAuth1\n",
    "api = TwitterAPI(API_KEY, \n",
    "                 API_KEY_SECRET, \n",
    "                 ACCESS_TOKEN, \n",
    "                 ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TwitterAPI.BearerAuth.BearerAuth at 0x115eeda00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## oAuth2 \n",
    "api = TwitterAPI(API_KEY,\n",
    "                 API_KEY_SECRET,\n",
    "                 auth_type='oAuth2')\n",
    "\n",
    "api.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter API will return a JSON object for each tweet, which contains a remarkable amount of information, over fifty fields. In the interest of saving space and not collecting more data than necesssary, we were selective in which fields we wanted to save, which will be listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information to be collected:\n",
    "\n",
    "We've decided on the following fields to be saved for a given Tweet:\n",
    "- 'tweet id' \n",
    "- 'text'\n",
    "- 'time'\n",
    "- 'tweet location'\n",
    "- 'hashtags'\n",
    "- 'likes'\n",
    "- 'retweets'\n",
    "- 'possibly sensitive'\n",
    "    - this is a notable field because it's interesting to see what Twitter classfies as possibly sensitive. The Tweets returned by the queries are pretty recent, so this category is likely not assigned due to manual review.\n",
    "- 'name'\n",
    "- 'screen name'\n",
    "- 'user location'\n",
    "- 'user bio'\n",
    "- 'flagged term' \n",
    "    - the term that we were looking for. See below for how we pick these terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are we going to find these Tweets with hateful speech?\n",
    "When doing research on previous work on hate speech online, we found [this csv](https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/lexicons/readme.md) containing a dictionary of words considered hateful. Some of these we don't necessarily agree with as being hate speech, such as \"Yankee\". However, it contains a lot of flagged phrases, some of which are slurs, others are more veiled, dog-whistle phrases such as \"border jumper\". \n",
    "\n",
    "note: we are considering using a different list. The source from which we got this file said it lead to a lot of false positives due to the presence of words like \"yellow\" and \"bird\" which have plenty of uses that are not hateful at all. If we think data analysis could benefit from a different list and we could find a good one online, we might use that, but we don't want to create our own list of words to search for by typing out slurs ourselves.\n",
    "\n",
    "The list needed some cleaning (shown below) due to extraneuous commas and quotation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_slurs = pd.read_csv('hatebase_dict.csv', index_col=0, header=None)\n",
    "\n",
    "slurs_list = []\n",
    "\n",
    "for item in df_slurs.iterrows():\n",
    "    slur = item[0]\n",
    "    # removing quote marks and commas\n",
    "    slur = slur.replace(\"'\", \"\")\n",
    "    slur = slur.replace(\",\", \"\")\n",
    "    slurs_list.append(slur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words in the Hatebase dictionary are not inherently hateful but are characteristic of a hateful attitude, such as \"uncivilised\". However, some are vulgar and incredibly offensive slurs, so we will not be printing the full contents of the dictionary here, but have included a few examples from it to provide a sense of the contents of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uncivilised', 'jungle bunny', 'paleface', 'mud persons', 'stovepipes')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurs_list[0], slurs_list[100], slurs_list[250], slurs_list[500], slurs_list[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(slurs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Overview\n",
    "We will accomplish this task in two parts. The first is the function `get_tweets_with_term`. This will return a dataframe the first 100 tweets that the a query to the API with the given term contains. The information listed above will be the columns of the dataframe.\n",
    "\n",
    "The second part of the pipeline is the function `get_tweets_containing_terms`, which takes in a list of terms and repeatedly calls `get_tweets_with_term`, building a dataframe as it goes. Because of concerns about hitting the maximum number of requests we can make to the API, we are currently not using this function and instead are using a script so that even if the API shuts us out, we will still have a partially completed dataframe. We are going to be collecting up to 100 tweets for 1,034 terms in the csv from Hatebase, so the dataframe could have up to 100,000 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the product I want to query (as opposed to the 7-day or 30-day archives)\n",
    "PRODUCT = 'fullarchive'\n",
    "# the label for the dev environment I set up on the Twitter developer portal\n",
    "LABEL = 'mydev'\n",
    "\n",
    "from TwitterAPI import TwitterPager\n",
    "\n",
    "def get_tweets_with_term(term):\n",
    "    \"\"\"\n",
    "    Finds up to 100 tweets in the full archive containing the given term.\n",
    "    Args:\n",
    "        term (str) : the string to search for in the archive\n",
    "    Returns:\n",
    "        df (dataframe) : a dataframe containing information on Tweets containing the given term\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    count = 0\n",
    "    # the maximum number of Tweets to be collected from this search\n",
    "    max_count = 100\n",
    "    \n",
    "    pager = TwitterPager(api, 'search/tweets', {'q': term, 'count': max_count})\n",
    "    # TwitterPager returns a JSON object\n",
    "    \n",
    "    # Twitter limits us to 300 requests per 15 minutes, so we have to wait three seconds\n",
    "    for item in pager.get_iterator(wait=5):\n",
    "        #print(json.dumps(item, indent=3, sort_keys=True))\n",
    "        \n",
    "        info_dict = dict()\n",
    "        info_dict['tweet id'] = item['id']\n",
    "        info_dict['text'] = item['text']\n",
    "        info_dict['time'] = item['created_at']\n",
    "        info_dict['tweet location'] = item['geo']\n",
    "        info_dict['hashtags'] = item['entities']['hashtags']\n",
    "        info_dict['likes'] = item['favorite_count']\n",
    "        info_dict['retweets'] = item['retweet_count'] \n",
    "        try:\n",
    "            # we ran into some KeyErrors here, so we hypothesize that this key\n",
    "            # is not present in every Tweet\n",
    "            info_dict['possibly sensitive'] = item['possibly_sensitive']\n",
    "        except:\n",
    "            info_dict['possibly sensitive'] = None\n",
    "        info_dict['name'] = item['user']['name']\n",
    "        info_dict['screen name'] = item['user']['screen_name']\n",
    "        info_dict['user location'] = item['user']['location']\n",
    "        info_dict['user bio'] = item['user']['description']\n",
    "        info_dict['flagged term'] = term\n",
    "        \n",
    "        count = count + 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        \n",
    "        df = df.append(pd.Series(info_dict), ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_containing_terms(df_flagged_tweets, terms):\n",
    "    \"\"\"\n",
    "    Collects tweets containing any of the terms in the given list\n",
    "    Args:\n",
    "        df_flagged_tweets (dataframe) : the initial datafr\n",
    "        terms (list) : all the terms (strings) that we want to search for in the Twitter archives\n",
    "    Returns:\n",
    "        df_flagged_tweets (dataframe) : a dataframe of any tweet containing the any of the given terms\n",
    "    \"\"\"\n",
    "    \n",
    "    for term in terms:\n",
    "        \n",
    "        df_this_term = get_tweets_with_term(term)\n",
    "        \n",
    "        df_flagged_tweets = df_flagged_tweets.append(df_this_term)\n",
    "        \n",
    "    return df_flagged_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a small test case we created to ensure our pipeline would work on a small list (names of three political figures) before we ran it on a list of over a thousand query items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flagged term</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>name</th>\n",
       "      <th>possibly sensitive</th>\n",
       "      <th>retweets</th>\n",
       "      <th>screen name</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet location</th>\n",
       "      <th>user bio</th>\n",
       "      <th>user location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harris</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rising serpent üá∫üá∏</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>rising_serpent</td>\n",
       "      <td>RT @TrumpJew2: THREAD: Kamala Harris doesn‚Äôt s...</td>\n",
       "      <td>Tue Mar 23 00:28:14 +0000 2021</td>\n",
       "      <td>1.374156e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>You can only fight the way you practice</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harris</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Press Secretary‚Äôs Ushanka</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>BeenLucky7</td>\n",
       "      <td>RT @TrumpJew2: THREAD: Kamala Harris doesn‚Äôt s...</td>\n",
       "      <td>Tue Mar 23 00:28:13 +0000 2021</td>\n",
       "      <td>1.374156e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>‚ÄúTruth is treason in the empire of lies‚Äù RP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harris</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dave Revcar</td>\n",
       "      <td>0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>DRevcar</td>\n",
       "      <td>RT @DailyCaller: ‚ÄúDo you have plans to visit t...</td>\n",
       "      <td>Tue Mar 23 00:28:13 +0000 2021</td>\n",
       "      <td>1.374156e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Liberalism, pray for the cure.\\nI got a like f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harris</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Anne Tirone</td>\n",
       "      <td>None</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>anne_tirone</td>\n",
       "      <td>RT @DearAuntCrabby: Republicans didn't give a ...</td>\n",
       "      <td>Tue Mar 23 00:28:10 +0000 2021</td>\n",
       "      <td>1.374156e+18</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harris</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kelly Allistone, CHRM ‚úçüèª</td>\n",
       "      <td>None</td>\n",
       "      <td>111.0</td>\n",
       "      <td>kellykba</td>\n",
       "      <td>RT @flywithkamala: This adorable set of triple...</td>\n",
       "      <td>Tue Mar 23 00:28:10 +0000 2021</td>\n",
       "      <td>1.374156e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Freelance copy editor and comma lover. Your qu...</td>\n",
       "      <td>Ontario, Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>fauci</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ü§°üåé</td>\n",
       "      <td>None</td>\n",
       "      <td>147.0</td>\n",
       "      <td>ClownWorld2020</td>\n",
       "      <td>RT @nedryun: Some of us back in April 2020 wer...</td>\n",
       "      <td>Tue Mar 23 00:24:05 +0000 2021</td>\n",
       "      <td>1.374155e+18</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>fauci</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Radio TFI (Home of The Taxi Stand Hour)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TheRadioTFI</td>\n",
       "      <td>Dr. Fauci On The Johnson &amp;amp; Johnson Vaccine...</td>\n",
       "      <td>Tue Mar 23 00:24:04 +0000 2021</td>\n",
       "      <td>1.374155e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Home of music, memories and more. As well as t...</td>\n",
       "      <td>Queens, NY - Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>fauci</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>curt (‚ßñ)</td>\n",
       "      <td>False</td>\n",
       "      <td>524.0</td>\n",
       "      <td>audiblevideo</td>\n",
       "      <td>RT @TeaPainUSA: This one arrogant decision kil...</td>\n",
       "      <td>Tue Mar 23 00:23:59 +0000 2021</td>\n",
       "      <td>1.374155e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Follow me through the Anthropocene</td>\n",
       "      <td>los angeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>fauci</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Itakepictures</td>\n",
       "      <td>None</td>\n",
       "      <td>5218.0</td>\n",
       "      <td>COskidiva</td>\n",
       "      <td>RT @kaitlancollins: \"I listened to him, but I ...</td>\n",
       "      <td>Tue Mar 23 00:23:57 +0000 2021</td>\n",
       "      <td>1.374155e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Photographer, skier, wife and mother! Trump is...</td>\n",
       "      <td>Boulder, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>fauci</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Samantha Schoening</td>\n",
       "      <td>None</td>\n",
       "      <td>5218.0</td>\n",
       "      <td>SamanthaSchoen7</td>\n",
       "      <td>RT @kaitlancollins: \"I listened to him, but I ...</td>\n",
       "      <td>Tue Mar 23 00:23:48 +0000 2021</td>\n",
       "      <td>1.374155e+18</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flagged term hashtags  likes                                     name  \\\n",
       "0        harris       []    0.0                        Rising serpent üá∫üá∏   \n",
       "1        harris       []    0.0                Press Secretary‚Äôs Ushanka   \n",
       "2        harris       []    0.0                              Dave Revcar   \n",
       "3        harris       []    0.0                              Anne Tirone   \n",
       "4        harris       []    0.0                 Kelly Allistone, CHRM ‚úçüèª   \n",
       "..          ...      ...    ...                                      ...   \n",
       "94        fauci       []    0.0                                       ü§°üåé   \n",
       "95        fauci       []    0.0  Radio TFI (Home of The Taxi Stand Hour)   \n",
       "96        fauci       []    0.0                                 curt (‚ßñ)   \n",
       "97        fauci       []    0.0                            Itakepictures   \n",
       "98        fauci       []    0.0                       Samantha Schoening   \n",
       "\n",
       "   possibly sensitive  retweets      screen name  \\\n",
       "0                   0      28.0   rising_serpent   \n",
       "1                   0      28.0       BeenLucky7   \n",
       "2                   0    1525.0          DRevcar   \n",
       "3                None    2760.0      anne_tirone   \n",
       "4                None     111.0         kellykba   \n",
       "..                ...       ...              ...   \n",
       "94               None     147.0   ClownWorld2020   \n",
       "95              False       0.0      TheRadioTFI   \n",
       "96              False     524.0     audiblevideo   \n",
       "97               None    5218.0        COskidiva   \n",
       "98               None    5218.0  SamanthaSchoen7   \n",
       "\n",
       "                                                 text  \\\n",
       "0   RT @TrumpJew2: THREAD: Kamala Harris doesn‚Äôt s...   \n",
       "1   RT @TrumpJew2: THREAD: Kamala Harris doesn‚Äôt s...   \n",
       "2   RT @DailyCaller: ‚ÄúDo you have plans to visit t...   \n",
       "3   RT @DearAuntCrabby: Republicans didn't give a ...   \n",
       "4   RT @flywithkamala: This adorable set of triple...   \n",
       "..                                                ...   \n",
       "94  RT @nedryun: Some of us back in April 2020 wer...   \n",
       "95  Dr. Fauci On The Johnson &amp; Johnson Vaccine...   \n",
       "96  RT @TeaPainUSA: This one arrogant decision kil...   \n",
       "97  RT @kaitlancollins: \"I listened to him, but I ...   \n",
       "98  RT @kaitlancollins: \"I listened to him, but I ...   \n",
       "\n",
       "                              time      tweet id tweet location  \\\n",
       "0   Tue Mar 23 00:28:14 +0000 2021  1.374156e+18           None   \n",
       "1   Tue Mar 23 00:28:13 +0000 2021  1.374156e+18           None   \n",
       "2   Tue Mar 23 00:28:13 +0000 2021  1.374156e+18           None   \n",
       "3   Tue Mar 23 00:28:10 +0000 2021  1.374156e+18           None   \n",
       "4   Tue Mar 23 00:28:10 +0000 2021  1.374156e+18           None   \n",
       "..                             ...           ...            ...   \n",
       "94  Tue Mar 23 00:24:05 +0000 2021  1.374155e+18           None   \n",
       "95  Tue Mar 23 00:24:04 +0000 2021  1.374155e+18           None   \n",
       "96  Tue Mar 23 00:23:59 +0000 2021  1.374155e+18           None   \n",
       "97  Tue Mar 23 00:23:57 +0000 2021  1.374155e+18           None   \n",
       "98  Tue Mar 23 00:23:48 +0000 2021  1.374155e+18           None   \n",
       "\n",
       "                                             user bio           user location  \n",
       "0             You can only fight the way you practice           United States  \n",
       "1         ‚ÄúTruth is treason in the empire of lies‚Äù RP                          \n",
       "2   Liberalism, pray for the cure.\\nI got a like f...                          \n",
       "3                                                                              \n",
       "4   Freelance copy editor and comma lover. Your qu...         Ontario, Canada  \n",
       "..                                                ...                     ...  \n",
       "94                                                                             \n",
       "95  Home of music, memories and more. As well as t...  Queens, NY - Worldwide  \n",
       "96                 Follow me through the Anthropocene             los angeles  \n",
       "97  Photographer, skier, wife and mother! Trump is...             Boulder, CO  \n",
       "98                                                                             \n",
       "\n",
       "[297 rows x 13 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame()\n",
    "people = ['harris', 'biden', 'fauci']\n",
    "\n",
    "for term in people:\n",
    "        \n",
    "    df_this_term = get_tweets_with_term(term)\n",
    "    \n",
    "    df_test = df_test.append(df_this_term)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe looks as expected, so we'll proceed with getting the data we're really interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "Here is where we run the pipeline on the whole list of slurs / hateful speech. This is an incredibly large amount of data. We are also saving it to a csv because it will take about an hour to make all the necessary queries to the Twitter API without breaking the limit, and we would rather not run it more times than necessary. We're not using a `_final.csv` structure because the only orignal csv is `hatebase_dict.csv`, which we are not modifying.\n",
    "\n",
    "The current csv is 20.3 MB, and it took about an hour to build.\n",
    "\n",
    "### CONTENT WARNING: \n",
    "### We have not censored Tweets that were scraped from the API at all, so this csv will contain many examples of harmful slurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flagged_tweets = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in slurs_list:\n",
    "        \n",
    "    df_this_term = get_tweets_with_term(term)\n",
    "    \n",
    "    df_flagged_tweets = df_flagged_tweets.append(df_this_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataframe as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flagged_tweets.to_csv('flagged_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flagged term</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>name</th>\n",
       "      <th>possibly sensitive</th>\n",
       "      <th>retweets</th>\n",
       "      <th>screen name</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet location</th>\n",
       "      <th>user bio</th>\n",
       "      <th>user location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncivilised</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Quincunx</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Quincun36705461</td>\n",
       "      <td>@dcvandiver @nancycruise1 uncivilised is how i...</td>\n",
       "      <td>Tue Mar 23 00:34:49 +0000 2021</td>\n",
       "      <td>1.374158e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>One of the silent majority who will not be sil...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uncivilised</td>\n",
       "      <td>[]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>British border in Ireland -Irish did‚Äônt make it.</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>border_ireland</td>\n",
       "      <td>@conormlally @ChrisMcNultyDgl Did you consider...</td>\n",
       "      <td>Tue Mar 23 00:14:01 +0000 2021</td>\n",
       "      <td>1.374152e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>It‚Äôs not an Irish border, Irish didn‚Äôt put it ...</td>\n",
       "      <td>The Border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uncivilised</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>evil_gnome üçÑ</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gnome_196883</td>\n",
       "      <td>@frontwheelskid They are supposed to be diplom...</td>\n",
       "      <td>Mon Mar 22 23:51:01 +0000 2021</td>\n",
       "      <td>1.374147e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>When I'm not sitting in front of a computer I ...</td>\n",
       "      <td>üá¨üáß üá≤üá∞ üá™üá™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uncivilised</td>\n",
       "      <td>[]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Peter üò∑</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pbmosligo</td>\n",
       "      <td>@Andrewm05562037 @JoeBrolly1993 What has no pl...</td>\n",
       "      <td>Mon Mar 22 23:30:44 +0000 2021</td>\n",
       "      <td>1.374142e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>- politics - current affairs - well being - bu...</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uncivilised</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Loop PNG</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>looppng</td>\n",
       "      <td>Southern Command‚Äôs Assistant Commissioner of P...</td>\n",
       "      <td>Mon Mar 22 23:14:06 +0000 2021</td>\n",
       "      <td>1.374137e+18</td>\n",
       "      <td>None</td>\n",
       "      <td>Loop is PNG's #1 digital news source for local...</td>\n",
       "      <td>Papua New Guinea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flagged term hashtags  likes  \\\n",
       "0  uncivilised       []    1.0   \n",
       "1  uncivilised       []    5.0   \n",
       "2  uncivilised       []    1.0   \n",
       "3  uncivilised       []    6.0   \n",
       "4  uncivilised       []    1.0   \n",
       "\n",
       "                                               name possibly sensitive  \\\n",
       "0                                          Quincunx               None   \n",
       "1  British border in Ireland -Irish did‚Äônt make it.               None   \n",
       "2                                      evil_gnome üçÑ               None   \n",
       "3                                           Peter üò∑               None   \n",
       "4                                          Loop PNG              False   \n",
       "\n",
       "   retweets      screen name  \\\n",
       "0       0.0  Quincun36705461   \n",
       "1       0.0   border_ireland   \n",
       "2       0.0     gnome_196883   \n",
       "3       0.0        pbmosligo   \n",
       "4       0.0          looppng   \n",
       "\n",
       "                                                text  \\\n",
       "0  @dcvandiver @nancycruise1 uncivilised is how i...   \n",
       "1  @conormlally @ChrisMcNultyDgl Did you consider...   \n",
       "2  @frontwheelskid They are supposed to be diplom...   \n",
       "3  @Andrewm05562037 @JoeBrolly1993 What has no pl...   \n",
       "4  Southern Command‚Äôs Assistant Commissioner of P...   \n",
       "\n",
       "                             time      tweet id tweet location  \\\n",
       "0  Tue Mar 23 00:34:49 +0000 2021  1.374158e+18           None   \n",
       "1  Tue Mar 23 00:14:01 +0000 2021  1.374152e+18           None   \n",
       "2  Mon Mar 22 23:51:01 +0000 2021  1.374147e+18           None   \n",
       "3  Mon Mar 22 23:30:44 +0000 2021  1.374142e+18           None   \n",
       "4  Mon Mar 22 23:14:06 +0000 2021  1.374137e+18           None   \n",
       "\n",
       "                                            user bio     user location  \n",
       "0  One of the silent majority who will not be sil...                    \n",
       "1  It‚Äôs not an Irish border, Irish didn‚Äôt put it ...        The Border  \n",
       "2  When I'm not sitting in front of a computer I ...         üá¨üáß üá≤üá∞ üá™üá™   \n",
       "3  - politics - current affairs - well being - bu...           Ireland  \n",
       "4  Loop is PNG's #1 digital news source for local...  Papua New Guinea  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flagged_tweets.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
